{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîÑ CNN Advanced Resume Notebook (–±–µ–∑ –≤—Ç—Ä–∞—Ç)\n",
        "\n",
        "–¶–µ–π –Ω–æ—É—Ç–±—É–∫ –ø—Ä–æ–¥–æ–≤–∂—É—î –ø–µ—Ä–µ—Ä–≤–∞–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è **CNN Advanced** –∑ `weights/advanced_last.pth` —ñ –∑–±–µ—Ä—ñ–≥–∞—î –ø–æ–ø–µ—Ä–µ–¥–Ω—é —ñ—Å—Ç–æ—Ä—ñ—é –µ–ø–æ—Ö.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ (Colab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –Ø–∫—â–æ –ø—Ä–∞—Ü—é—î—à —É Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üì¶ Installing dependencies...\")\n",
        "!pip install -q torch torchvision torchaudio pandas numpy tqdm matplotlib\n",
        "print(\"‚úÖ Dependencies installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22eab7bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"üî• CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e180e2f8",
      "metadata": {},
      "source": [
        "## 2) –®–ª—è—Ö–∏ —Ç–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ñ–∞–π–ª—ñ–≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98f8e8e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–º—ñ–Ω–∏ —à–ª—è—Ö –ø—ñ–¥ —Å–≤—ñ–π Drive\n",
        "PROJECT_PATH = '/content/drive/MyDrive/diplom/backend'\n",
        "os.chdir(PROJECT_PATH)\n",
        "print(\"üìÇ Working directory:\", os.getcwd())\n",
        "\n",
        "required_files = ['train.py', 'dataset.py', 'models/__init__.py', 'data/sudoku.csv']\n",
        "for file in required_files:\n",
        "    print(f\"‚úÖ {file}\" if os.path.exists(file) else f\"‚ùå {file} - NOT FOUND\")\n",
        "\n",
        "os.makedirs('weights', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f879f44",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîç –ü–ï–†–ï–í–Ü–†–ö–ê –í–ï–†–°–Ü–á train.py (–º–∞—î –±—É—Ç–∏ –ë–ï–ó eager imports)\n",
        "print(\"üîç Checking train.py version...\")\n",
        "with open('train.py', 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä—è–¥–∫–∏ 16-18\n",
        "    has_old_import = any('from models import CNNBaseline, CNNAdvanced, GNNModel, SudokuRNN' in line for line in lines[15:20])\n",
        "    \n",
        "    if has_old_import:\n",
        "        print(\"‚ùå –°–¢–ê–†–ê –≤–µ—Ä—Å—ñ—è train.py –≤–∏—è–≤–ª–µ–Ω–∞!\")\n",
        "        print(\"   –§–∞–π–ª –º–∞—î eager import –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π, —â–æ –≤–∏–∫–ª–∏–∫–∞—î torch_geometric –ø–æ–º–∏–ª–∫—É.\")\n",
        "        print(\"   \")\n",
        "        print(\"   –†–Ü–®–ï–ù–ù–Ø: –°–∏–Ω—Ö—Ä–æ–Ω—ñ–∑—É–π –æ–Ω–æ–≤–ª–µ–Ω–∏–π train.py –∑ –ª–æ–∫–∞–ª—å–Ω–æ—ó –º–∞—à–∏–Ω–∏ –≤ Google Drive,\")\n",
        "        print(\"            –∞–±–æ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏ Runtime: Runtime ‚Üí Restart runtime\")\n",
        "        raise RuntimeError(\"train.py –ø–æ—Ç—Ä–µ–±—É—î –æ–Ω–æ–≤–ª–µ–Ω–Ω—è! –î–∏–≤. COLAB_SYNC_INSTRUCTIONS.md\")\n",
        "    else:\n",
        "        print(\"‚úÖ train.py –≤–µ—Ä—Å—ñ—è –∫–æ—Ä–µ–∫—Ç–Ω–∞ (lazy imports)\")\n",
        "        print(\"   –ú–æ–¥–µ–ª—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—é—Ç—å—Å—è —Ç—ñ–ª—å–∫–∏ –∫–æ–ª–∏ –ø–æ—Ç—Ä—ñ–±–Ω—ñ - torch_geometric –Ω–µ –ø–æ—Ç—Ä—ñ–±–µ–Ω –¥–ª—è CNN Advanced\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a99dc8c",
      "metadata": {},
      "source": [
        "## 3) Resume-–∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è\n",
        "\n",
        "> `total_epochs` ‚Äî —Ü–µ **–∑–∞–≥–∞–ª—å–Ω–∞ —Ü—ñ–ª—å** –Ω–∞–≤—á–∞–Ω–Ω—è (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥ 40), –∞ –Ω–µ \"—â–µ –¥–æ–∫–∏–Ω—É—Ç–∏ N –µ–ø–æ—Ö\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1df2d98",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === –°–¢–ê–ù–î–ê–†–¢–ù–Ü –ü–ê–†–ê–ú–ï–¢–†–ò CNN ADVANCED ===\n",
        "TOTAL_EPOCHS = 40\n",
        "BATCH_SIZE = 64\n",
        "LR = 0.001\n",
        "WEIGHT_DECAY = 1e-5\n",
        "HIDDEN_CHANNELS = 128\n",
        "NUM_RESIDUAL_BLOCKS = 20\n",
        "NUM_WORKERS = 2\n",
        "LIMIT = 1000000\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "CKPT_LAST = 'weights/advanced_last.pth'\n",
        "CKPT_BEST = 'weights/advanced_best.pth'\n",
        "HISTORY_PATH = 'weights/advanced_history.json'\n",
        "HISTORY_BACKUP = f\"weights/advanced_history_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "\n",
        "print('Device:', DEVICE)\n",
        "print('Checkpoint last exists:', os.path.exists(CKPT_LAST))\n",
        "print('History exists:', os.path.exists(HISTORY_PATH))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e3230a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó—á–∏—Ç—É—î–º–æ —Å—Ç–∞–Ω –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º\n",
        "initial_history = []\n",
        "if os.path.exists(HISTORY_PATH):\n",
        "    with open(HISTORY_PATH, 'r') as f:\n",
        "        initial_history = json.load(f)\n",
        "    shutil.copy2(HISTORY_PATH, HISTORY_BACKUP)\n",
        "    print(f\"üõü Backup history: {HISTORY_BACKUP}\")\n",
        "\n",
        "start_epoch = 1\n",
        "resume_mismatch_warning = None\n",
        "if os.path.exists(CKPT_LAST):\n",
        "    try:\n",
        "        ckpt = torch.load(CKPT_LAST, map_location='cpu')\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Standard torch.load failed: {e}\")\n",
        "        print(\"Retrying with weights_only=False for trusted local checkpoint...\")\n",
        "        ckpt = torch.load(CKPT_LAST, map_location='cpu', weights_only=False)\n",
        "\n",
        "    start_epoch = int(ckpt['epoch']) + 1\n",
        "    print(f\"üîÑ Resume from epoch {start_epoch} (last finished: {ckpt['epoch']})\")\n",
        "\n",
        "    ckpt_args = ckpt.get('args', {}) if isinstance(ckpt, dict) else {}\n",
        "    expected = {\n",
        "        'model': 'advanced',\n",
        "        'hidden_channels': HIDDEN_CHANNELS,\n",
        "        'num_residual_blocks': NUM_RESIDUAL_BLOCKS,\n",
        "    }\n",
        "    mismatches = []\n",
        "    for k, v in expected.items():\n",
        "        if k in ckpt_args and ckpt_args[k] != v:\n",
        "            mismatches.append(f\"{k}: ckpt={ckpt_args[k]} != current={v}\")\n",
        "\n",
        "    if mismatches:\n",
        "        resume_mismatch_warning = \"; \".join(mismatches)\n",
        "        print(\"‚ö†Ô∏è Checkpoint config mismatch detected:\")\n",
        "        for item in mismatches:\n",
        "            print(\"   -\", item)\n",
        "        print(\"   Training may fail on --resume. Consider deleting CKPT_LAST or changing params.\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è Checkpoint not found: training from scratch\")\n",
        "\n",
        "if start_epoch > TOTAL_EPOCHS:\n",
        "    print(f\"‚úÖ –ù–∞–≤—á–∞–Ω–Ω—è –≤–∂–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: checkpoint –º–∞—î epoch {start_epoch - 1}, —Ü—ñ–ª—å {TOTAL_EPOCHS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97c887f1",
      "metadata": {},
      "source": [
        "## 4) –ó–∞–ø—É—Å–∫ –Ω–∞–≤—á–∞–Ω–Ω—è (resume –∞–±–æ fresh)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eacacc79",
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –Ω–∞–≤—á–∞–Ω–Ω—è\n",
        "if start_epoch > TOTAL_EPOCHS:\n",
        "    print(f\"‚è≠Ô∏è Training already completed (epoch {start_epoch - 1}/{TOTAL_EPOCHS})\")\n",
        "    skip_training = True\n",
        "else:\n",
        "    skip_training = False\n",
        "    print(f\"üöÄ Will train epochs {start_epoch} to {TOTAL_EPOCHS}\")\n",
        "    if os.path.exists(CKPT_LAST):\n",
        "        print(f\"   Resume: Yes (from checkpoint)\")\n",
        "    else:\n",
        "        print(f\"   Resume: No (fresh start)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad7c21e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === –ù–ê–í–ß–ê–ù–ù–Ø CNN ADVANCED (RESUME) ===\n",
        "# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î ! –¥–ª—è real-time –≤–∏–≤–æ–¥—É –≤ Colab\n",
        "\n",
        "if not skip_training:\n",
        "    !python -u train.py \\\n",
        "        --model advanced \\\n",
        "        --data data/sudoku.csv \\\n",
        "        --epochs {TOTAL_EPOCHS} \\\n",
        "        --batch-size {BATCH_SIZE} \\\n",
        "        --lr {LR} \\\n",
        "        --weight-decay {WEIGHT_DECAY} \\\n",
        "        --hidden-channels {HIDDEN_CHANNELS} \\\n",
        "        --num-residual-blocks {NUM_RESIDUAL_BLOCKS} \\\n",
        "        --num-workers {NUM_WORKERS} \\\n",
        "        --limit {LIMIT} \\\n",
        "        --save-dir weights \\\n",
        "        --device {DEVICE} \\\n",
        "        --resume {CKPT_LAST if os.path.exists(CKPT_LAST) else ''}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –ü–û–í–ù–û–á —ñ—Å—Ç–æ—Ä—ñ—ó –µ–ø–æ—Ö (—â–æ–± –Ω–µ –≤—Ç—Ä–∞—Ç–∏—Ç–∏ —Ä–∞–Ω–Ω—ñ –µ–ø–æ—Ö–∏)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train.py –ø—Ä–∏ resume –ø–µ—Ä–µ–∑–∞–ø–∏—Å—É—î history (–Ω–µ –¥–æ–ø–∏—Å—É—î).\n",
        "# –¢—É—Ç –º–∏ –∑–ª–∏–≤–∞—î–º–æ —Å—Ç–∞—Ä—É + –Ω–æ–≤—É —ñ—Å—Ç–æ—Ä—ñ—é –ø–æ –Ω–æ–º–µ—Ä—É epoch.\n",
        "\n",
        "merged = {}\n",
        "for item in initial_history:\n",
        "    merged[int(item['epoch'])] = item\n",
        "\n",
        "if os.path.exists(HISTORY_PATH):\n",
        "    with open(HISTORY_PATH, 'r') as f:\n",
        "        new_history = json.load(f)\n",
        "    for item in new_history:\n",
        "        merged[int(item['epoch'])] = item\n",
        "\n",
        "full_history = [merged[k] for k in sorted(merged.keys())]\n",
        "\n",
        "with open(HISTORY_PATH, 'w') as f:\n",
        "    json.dump(full_history, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Full history epochs: {len(full_history)}\")\n",
        "if full_history:\n",
        "    print(f\"Epoch range: {full_history[0]['epoch']}..{full_history[-1]['epoch']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) –ì—Ä–∞—Ñ—ñ–∫–∏ —Ç–∞ —Ñ—ñ–Ω–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(HISTORY_PATH, 'r') as f:\n",
        "    history = json.load(f)\n",
        "\n",
        "if not history:\n",
        "    raise ValueError(\"History is empty. Check training run / checkpoints.\")\n",
        "\n",
        "epochs = [h['epoch'] for h in history]\n",
        "train_loss = [h['train']['loss'] for h in history]\n",
        "val_loss = [h['val']['loss'] for h in history]\n",
        "val_acc = [h['val']['cell_accuracy'] for h in history]\n",
        "board_acc = [h['val']['board_accuracy'] for h in history]\n",
        "lr_hist = [h['lr'] for h in history]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "axes[0].plot(epochs, train_loss, label='Train Loss')\n",
        "axes[0].plot(epochs, val_loss, label='Val Loss')\n",
        "axes[0].set_title('Loss')\n",
        "axes[0].grid(True)\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(epochs, val_acc, label='Val Cell Accuracy', color='tab:blue')\n",
        "axes[1].plot(epochs, board_acc, label='Val Board Accuracy', color='tab:green')\n",
        "axes[1].set_title('Accuracy')\n",
        "axes[1].grid(True)\n",
        "axes[1].legend()\n",
        "\n",
        "axes[2].plot(epochs, lr_hist, label='LR', color='tab:red')\n",
        "axes[2].set_title('Learning rate')\n",
        "axes[2].set_yscale('log')\n",
        "axes[2].grid(True)\n",
        "axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('weights/advanced_resume_curves.png', dpi=250, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('\\nüìä FINAL SUMMARY')\n",
        "print('Best Val Loss:      ', min(val_loss))\n",
        "print('Best Cell Accuracy: ', max(val_acc))\n",
        "print('Best Board Accuracy:', max(board_acc))\n",
        "print('Last Epoch:         ', epochs[-1])\n",
        "print('Saved plot: weights/advanced_resume_curves.png')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
